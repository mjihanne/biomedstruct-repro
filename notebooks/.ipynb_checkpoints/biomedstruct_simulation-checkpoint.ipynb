{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b117e6f5-1fb1-4979-bc67-301e631c6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BioMedStruct: end-to-end SYNTHETIC SIMULATION \n",
    "# ============================================================\n",
    "# This notebook demonstrates the complete BioMedStruct pipeline\n",
    "# on fully synthetic and randomly generated maintenance data.\n",
    "# No real hospital data are used or reproduced.\n",
    "#\n",
    "# Stages:\n",
    "# M1 – Source ingestion + provenance tracking\n",
    "# M2 – Data cleaning and logical consistency\n",
    "# M3 – Semantic normalization\n",
    "# M4 – Feature engineering (leakage-safe)\n",
    "# M5 – Quality gates\n",
    "# M6 – Final structured dataset\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0) Synthetic heterogeneous maintenance data generation\n",
    "# ------------------------------------------------------------\n",
    "root = Path(\"./sim_biomedstruct\")\n",
    "raw_dir = root / \"data_raw\"\n",
    "out_dir = root / \"out\"\n",
    "raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "base_date = pd.Timestamp(\"2015-01-01\")\n",
    "\n",
    "# Synthetic Site A (spreadsheet-style, FR headers)\n",
    "dfA = pd.DataFrame({\n",
    "    \"Service\": [\"Unit_A\", \"Unit_B\", \"Unit_C\", \"Unit_B\"],\n",
    "    \"Désignation équipement\": [\"Device_X\", \"Device_Y\", \"Device_Z\", \"Device_W\"],\n",
    "    \"Marque\": [\"Brand_A\", \"Brand_B\", \"Brand_C\", \"Brand_B\"],\n",
    "    \"Modèle/Type\": [\"Model_1\", \"Model_2\", \"Model_3\", \"Model_4\"],\n",
    "    \"N° Inventaire\": [f\"INV_{i:04d}\" for i in rng.integers(100, 999, size=4)],\n",
    "    \"Date panne\": [(base_date + pd.Timedelta(days=int(d))).strftime(\"%d/%m/%Y\")\n",
    "                   for d in rng.integers(0, 300, size=4)],\n",
    "    \"Date d’intervention\": [(base_date + pd.Timedelta(days=int(d))).strftime(\"%d/%m/%Y\")\n",
    "                            for d in rng.integers(1, 320, size=4)],\n",
    "    \"Type de panne\": [\"Type_A\", \"Type_B\", \"Type_C\", \"Type_A\"],\n",
    "    \"Etat\": [\"ok\", \"ok\", \"pending\", \"ok\"],\n",
    "    \"Nature d intervention\": [\"internal\", \"internal\", \"internal\", \"internal\"],\n",
    "})\n",
    "\n",
    "# Synthetic Site B (CSV-style, EN headers, different formats)\n",
    "dfB = pd.DataFrame({\n",
    "    \"service\": [\"unit_a\", \"unit_b\", \"unit_b\"],\n",
    "    \"designation\": [\"Device_X\", \"Device_Y\", \"Device_W\"],\n",
    "    \"marque\": [\"Brand_A\", \"Brand_B\", \"Brand_B\"],\n",
    "    \"modele\": [\"Model_1\", \"Model_2\", \"Model_4\"],\n",
    "    \"inv_id\": [f\"INV_{i:04d}\" for i in rng.integers(100, 999, size=3)],\n",
    "    \"failure_date\": [(base_date + pd.Timedelta(days=int(d))).strftime(\"%Y-%m-%d\")\n",
    "                     for d in rng.integers(0, 300, size=3)],\n",
    "    \"date_interv\": [(base_date + pd.Timedelta(days=int(d))).strftime(\"%Y-%m-%d\")\n",
    "                    for d in rng.integers(1, 320, size=3)],\n",
    "    \"failure_type\": [\"Type_A\", \"Type_B\", \"Type_A\"],\n",
    "    \"status\": [\"ok\", \"ok\", \"ok\"],\n",
    "    \"nature_interv\": [\"internal\", \"internal\", \"internal\"]\n",
    "})\n",
    "\n",
    "dfA.to_excel(raw_dir / \"siteA_synthetic.xlsx\", index=False)\n",
    "dfB.to_csv(raw_dir / \"siteB_synthetic.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Helpers: hashing and column harmonization\n",
    "# ------------------------------------------------------------\n",
    "def sha256_file(p: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(p, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1 << 20), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "CANON_COLS = {\n",
    "    \"service\": [\"service\"],\n",
    "    \"designation\": [\"désignation équipement\", \"designation\"],\n",
    "    \"marque\": [\"marque\"],\n",
    "    \"modele\": [\"modèle/type\", \"modele\"],\n",
    "    \"inv_id\": [\"n° inventaire\", \"inv_id\"],\n",
    "    \"date_panne\": [\"date panne\", \"failure_date\"],\n",
    "    \"date_interv\": [\"date d’intervention\", \"date_interv\"],\n",
    "    \"type_panne\": [\"type de panne\", \"failure_type\"],\n",
    "    \"etat\": [\"etat\", \"status\"],\n",
    "    \"nature_interv\": [\"nature d intervention\", \"nature_interv\"],\n",
    "}\n",
    "\n",
    "def normalize_cols(cols):\n",
    "    return [str(c).strip().lower().replace(\"’\", \"'\") for c in cols]\n",
    "\n",
    "def map_to_canon(col):\n",
    "    for k, aliases in CANON_COLS.items():\n",
    "        if col in aliases:\n",
    "            return k\n",
    "    return col\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) M1 – Ingestion + provenance\n",
    "# ------------------------------------------------------------\n",
    "frames, prov_log = [], []\n",
    "\n",
    "for p in raw_dir.glob(\"*\"):\n",
    "    if p.suffix in [\".xlsx\", \".xls\"]:\n",
    "        xl = pd.ExcelFile(p)\n",
    "        for sh in xl.sheet_names:\n",
    "            df = xl.parse(sh, dtype=str)\n",
    "            df.columns = normalize_cols(df.columns)\n",
    "            df = df.rename(columns={c: map_to_canon(c) for c in df.columns})\n",
    "            df[\"__source_file\"] = p.name\n",
    "            df[\"__source_sheet\"] = sh\n",
    "            df[\"__ingest_time\"] = pd.Timestamp.utcnow().isoformat()\n",
    "            df[\"__file_hash\"] = sha256_file(p)\n",
    "            frames.append(df)\n",
    "            prov_log.append({\"file\": p.name, \"sheet\": sh, \"hash\": df[\"__file_hash\"].iloc[0]})\n",
    "    else:\n",
    "        df = pd.read_csv(p, dtype=str)\n",
    "        df.columns = normalize_cols(df.columns)\n",
    "        df = df.rename(columns={c: map_to_canon(c) for c in df.columns})\n",
    "        df[\"__source_file\"] = p.name\n",
    "        df[\"__source_sheet\"] = None\n",
    "        df[\"__ingest_time\"] = pd.Timestamp.utcnow().isoformat()\n",
    "        df[\"__file_hash\"] = sha256_file(p)\n",
    "        frames.append(df)\n",
    "        prov_log.append({\"file\": p.name, \"sheet\": None, \"hash\": df[\"__file_hash\"].iloc[0]})\n",
    "\n",
    "raw = pd.concat(frames, ignore_index=True)\n",
    "pd.DataFrame(prov_log).to_json(out_dir / \"provenance_log.jsonl\",\n",
    "                               orient=\"records\", lines=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) M2 – Cleaning and temporal consistency\n",
    "# ------------------------------------------------------------\n",
    "def parse_date(x):\n",
    "    try:\n",
    "        return pd.to_datetime(x, dayfirst=True, errors=\"coerce\")\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "for c in [\"date_panne\", \"date_interv\"]:\n",
    "    raw[c] = raw[c].apply(parse_date)\n",
    "\n",
    "raw = raw.sort_values([\"inv_id\", \"date_panne\", \"date_interv\"])\n",
    "raw = raw.drop_duplicates(subset=[\"inv_id\", \"date_panne\"], keep=\"first\")\n",
    "raw = raw[raw[\"date_interv\"] >= raw[\"date_panne\"]]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) M3 – Semantic normalization\n",
    "# ------------------------------------------------------------\n",
    "raw[\"service\"] = raw[\"service\"].str.upper()\n",
    "raw[\"type_panne\"] = raw[\"type_panne\"].str.upper()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) M4 – Feature engineering (leakage-safe)\n",
    "# ------------------------------------------------------------\n",
    "raw[\"mttr_days\"] = (raw[\"date_interv\"] - raw[\"date_panne\"]).dt.days\n",
    "\n",
    "raw = raw.sort_values([\"inv_id\", \"date_panne\"])\n",
    "raw[\"pannes_90j\"] = (\n",
    "    raw.groupby(\"inv_id\")[\"date_panne\"]\n",
    "    .rolling(\"90D\", on=\"date_panne\")\n",
    "    .count()\n",
    "    .reset_index(level=0, drop=True)\n",
    "    - 1\n",
    ").fillna(0)\n",
    "\n",
    "raw[\"criticality_score\"] = raw[\"pannes_90j\"] * raw[\"mttr_days\"].fillna(0)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) M5 – Quality gates\n",
    "# ------------------------------------------------------------\n",
    "quality_gates = {\n",
    "    \"temporal_consistency\": (raw[\"date_interv\"] >= raw[\"date_panne\"]).all(),\n",
    "    \"unique_events\": not raw.duplicated(subset=[\"inv_id\", \"date_panne\"]).any(),\n",
    "    \"feature_availability\": raw[\"mttr_days\"].notna().mean() >= 0.8\n",
    "}\n",
    "\n",
    "print(\"Quality gates:\", quality_gates)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7) M6 – Final structured dataset\n",
    "# ------------------------------------------------------------\n",
    "final_cols = [\n",
    "    \"service\", \"designation\", \"marque\", \"modele\", \"inv_id\",\n",
    "    \"date_panne\", \"date_interv\", \"type_panne\",\n",
    "    \"mttr_days\", \"pannes_90j\", \"criticality_score\",\n",
    "    \"__source_file\", \"__source_sheet\", \"__ingest_time\", \"__file_hash\"\n",
    "]\n",
    "\n",
    "final_df = raw[final_cols]\n",
    "final_df.to_csv(out_dir / \"BioMedStruct_synthetic_structured_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Final dataset shape:\", final_df.shape)\n",
    "final_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
